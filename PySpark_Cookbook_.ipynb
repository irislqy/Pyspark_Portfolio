{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark Cookbook .ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "giCtEEjgOydA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6jQzYo1lX8Y",
        "colab_type": "text"
      },
      "source": [
        "#Spark with Machine Learning Notebook\n",
        "\n",
        "This is my notebook for taking the DataCamp [Machine Learning with Apache Spark](https://www.datacamp.com/courses/machine-learning-with-apache-spark).\n",
        "\n",
        "PySpark.sql deals with structured data. There are more Important classes of Spark SQL. Please refer to [PySpark.sql module]( https://spark.apache.org/docs/2.3.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJaYV3lPTBIt",
        "colab_type": "text"
      },
      "source": [
        "##Content\n",
        "\n",
        "\n",
        "\n",
        "*   Load & Prepare Data\n",
        "*   Classifiers\n",
        "      1.   Decision Tree\n",
        "      2.   Logistic Regression\n",
        "\n",
        "*   Regression\n",
        "       1.  Linear Regression\n",
        "       2.  Penalized Regression\n",
        "*   Pipelines\n",
        "*   Cross-validation & grid search\n",
        "*   Ensembles\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giCtEEjgOydA",
        "colab_type": "text"
      },
      "source": [
        "##Preparation and Connecting to Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-4h8khfM2dH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install pyspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUk6HDJuNhwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyspark\n",
        "pyspark.__version__\n",
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53LqmoWoOxWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create a local cluster\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "                    .master('local[*]') \\\n",
        "                    .appName('test') \\\n",
        "                    .getOrCreate()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EhksoXBPVLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRqZIzEqSBh4",
        "colab_type": "text"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yMpU3TqeVsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "\"\"\"selected methods\"\"\"\n",
        "count()\n",
        "show()\n",
        "printSchema()\n",
        "\n",
        "\"\"\"Selected attributes, check column data types\"\"\"\n",
        "Dataset.dtypes\n",
        "\n",
        "\"\"\"Manual Set\"\"\"\n",
        "schema = StrucType([\n",
        "    StructField(\"maker\", StringType()),\n",
        "    StructField(\"cyl\", IntegerType()),\n",
        "    StructField(\"size\",DoubleType()),\n",
        "]\n",
        "\n",
        "cars = spark.readcsv(\"cars.csv\",header = True, schema = schema, nullValue ='NA')\n",
        "\n",
        "\n",
        "\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0IapqBDbscA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read data from CSV file\n",
        "# nullValue is case sensitive\n",
        "flights = spark.read.csv('flights.csv',\n",
        "                         sep=',',\n",
        "                         header=True,\n",
        "                         inferSchema=True,\n",
        "                         nullValue='NA')\n",
        "\n",
        "# Get number of records\n",
        "print(\"The data contain %d records.\" % flights.count())\n",
        "\n",
        "# View the first five records\n",
        "flights.show(5)\n",
        "\n",
        "# Check column data types\n",
        "flights.dtype"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZfDR3kVmjm_",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBic4Amamk8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"select the columns\"\n",
        "cars = cars.drop(\"\")\n",
        "cars = cars.select(\"\")\n",
        "\n",
        "\"count missing values\"\n",
        "cars.filter(\"cyl IS NULL\").count()\n",
        "cars = ars.filter('cyl IS NOT NULL')\n",
        "cars = cars.dropna()\n",
        "\n",
        "\"mutating columns\"\n",
        "from pyspark.sql.functions import round\n",
        "cars = cars.withColumn(\"mass\", round(cars.weight / 2,0))\n",
        "cars = cars.withColumn(\"length\", round(cars.length * 0.0254, 3))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"index categorical data\"\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "inder = StringIndexer(inputCol = 'type',outputCol = 'type_idx')\n",
        "\n",
        "#assign index values to strings\n",
        "#default, most frequent - 0, least frequent - 5\n",
        "# Use stringOderType to change order\n",
        "indexer = indexer.fit(cars)\n",
        "#create column with index values\n",
        "cars = indexer.transform(cars)\n",
        "\n",
        "#create new columns based on original column\n",
        "flights_km = flights_km.withColumn('label', (flights_km.delay >= 15).cast('integer'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"index categorical data - flights data\"\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Create an indexer\n",
        "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
        "\n",
        "# Indexer identifies categories in the data\n",
        "indexer_model = indexer.fit(flights)\n",
        "\n",
        "# Indexer creates a new column with numeric index values\n",
        "flights_indexed = indexer_model.transform(flights)\n",
        "\n",
        "# Repeat the process for the other categorical feature\n",
        "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"Assembling columns\"\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols = ['cyl','size'], outputCol = 'features')\n",
        "assember.transform(cars)\n",
        "\n",
        "# flights case \n",
        "# Import the necessary class\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Create an assembler object\n",
        "assembler = VectorAssembler(inputCols=['mon','dom','dow','carrier_idx','org_idx','km','depart','duration'], outputCol='features')\n",
        "\n",
        "# Consolidate predictor columns\n",
        "flights_assembled = assembler.transform(flights)\n",
        "\n",
        "# Check the resulting column\n",
        "flights_assembled.select('features', 'delay').show(5, truncate=False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQKmmFxxTEJj",
        "colab_type": "text"
      },
      "source": [
        "##Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2X8hD5ExuaE",
        "colab_type": "text"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzOuDuaOxt4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#specify a seed for reproducibility \n",
        "cars_train, cars_test = cars.randomSplit([0.8,0.2],seed = 23)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd2Xd-wEzPs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import DecisionTreeCalssifier\n",
        "\n",
        "tree = DecisionTreeClassifier()\n",
        "tree= tree.fit(cars_train)\n",
        "\n",
        "#evaluate\n",
        "prediction = tree.transform(cars_test)\n",
        "\n",
        "#confusion matrix\n",
        "prediction.groupBy(\"label\",\"prediction\").count().show()\n",
        "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
        "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
        "FN = prediction.filter('prediction = 0 AND label =1').count()\n",
        "FP = prediction.filter('prediction=1 AND label = 0').count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnHbRmai5KLi",
        "colab_type": "text"
      },
      "source": [
        "##Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqhiyKQr5Zsr",
        "colab_type": "text"
      },
      "source": [
        "Precision = TP/(TP+FP)\n",
        "Recall = TP/(TP+FN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9ioTCB45Lwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "logistic = LogisticRegression()\n",
        "logistic = logistic.fit(cars_train)\n",
        "\n",
        "prediction = logistic.transform(cars_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8npJTdr71lq",
        "colab_type": "text"
      },
      "source": [
        "**weightedRecall, accuracy, f1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUY8_l4C7gYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"weighted metrics\"\"\"\n",
        "\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator()\n",
        "evaluator.evalute(prediction, {evaluator.metricName:'weightedPrecision'})\n",
        "\n",
        "binary_evaluator = BinaryClassificationEvaluator()\n",
        "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName:'areaUnderROC'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00DmDeNi-o90",
        "colab_type": "text"
      },
      "source": [
        "##Turning Text into Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOYWm08K70vR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"remove punctuation\"\"\"\n",
        "from pyspark.sql.functions import regexp_replace\n",
        "\n",
        "REGEX = '[,\\\\-]'\n",
        "\n",
        "books = books.withColumn('text',regexp_replace(books.text, REGEX, ' '))\n",
        "\n",
        "\"\"\"text to tokens\"\"\"\n",
        "\n",
        "from pyspark.ml.feature import Tokenizer\n",
        "books = Tokenizer(inputCol = 'Text', outputCol = 'tokens').transform(books)\n",
        "\n",
        "\n",
        "\"\"\"stop words\"\"\"\n",
        "\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "stopwords = StopWordsRemover()\n",
        "stopwords.getStopWords()\n",
        "\n",
        "\"\"\"remove stop words\"\"\"\n",
        "stopwords = stopwords(setInputCol('tokens'),setOutputCol('words'))\n",
        "book = stopwords.transform(books)\n",
        "\n",
        "\"\"\"feature hasing\"\"\"\n",
        "\"32 is the largest value for numFeature\"\n",
        "\n",
        "from pyspark.ml.feature import HashingTF\n",
        "hasher = HashingTF(inputCol = \"words\", outputCol = \"hash\", numFeature = 32)\n",
        "books = hasher.transform(books)\n",
        "\n",
        "from pyspark.ml.feature import IDF \n",
        "\"inverse document frequency - account for frequency\"\n",
        "books = IDF(inputCol = 'hash',outputCol = \"features\").fit(books).transform(books)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRFU_BU-MI9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Import the necessary functions\n",
        "from pyspark.sql.functions import regexp_replace\n",
        "from pyspark.ml.feature import Tokenizer\n",
        "\n",
        "# Remove punctuation (REGEX provided) and numbers\n",
        "wrangled = sms.withColumn('text', regexp_replace(sms.text, '[_():;,.!?\\\\-]', ' '))\n",
        "wrangled = wrangled.withColumn('text',regexp_replace(wrangled.text, '[0-9]', ' '))\n",
        "\n",
        "# Merge multiple spaces\n",
        "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, ' +', ' '))\n",
        "\n",
        "# Split the text into words\n",
        "wrangled = Tokenizer(inputCol='text', outputCol='words').transform(wrangled)\n",
        "\n",
        "wrangled.show(4, truncate=False)\n",
        "\n",
        "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF\n",
        "\n",
        "# Remove stop words.\n",
        "\n",
        "wrangled = StopWordsRemover(inputCol='words', outputCol='terms')\\\n",
        "      .transform(sms)\n",
        "\n",
        "# Apply the hashing trick\n",
        "wrangled = HashingTF(inputCol = 'terms', outputCol = 'hash', numFeatures=1024)\\\n",
        "      .transform(wrangled)\n",
        "\n",
        "# Convert hashed symbols to TF-IDF\n",
        "tf_idf = IDF(inputCol = 'hash', outputCol = 'features')\\\n",
        "      .fit(wrangled).transform(wrangled)\n",
        "      \n",
        "tf_idf.select('terms', 'features').show(4, truncate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDugvHkQSQJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"train a spam classifier\"\n",
        "# Split the data into training and testing sets\n",
        "sms_train, sms_test = sms.randomSplit([0.8,0.2], seed = 13)\n",
        "\n",
        "# Fit a Logistic Regression model to the training data\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "logistic = LogisticRegression(regParam=0.2).fit(sms_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "prediction = logistic.transform(sms_test)\n",
        "\n",
        "# Create a confusion matrix, comparing predictions to known labels\n",
        "prediction.groupBy(\"label\", \"prediction\").count().show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzECBSbeSuot",
        "colab_type": "text"
      },
      "source": [
        "##Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dafJnQmS9YK",
        "colab_type": "text"
      },
      "source": [
        "##One-Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR2lKxuxSRlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoderEstimator\n",
        "\n",
        "onehoe = OneHotEncoderEtimator(inputCols = ['type_idx'],outputCols =['type_dummy'])\n",
        "\n",
        "onehot = onehot.fit(cars)\n",
        "\n",
        "\"how many category levels\"\n",
        "onehot.categorySizes\n",
        "\n",
        "cars = onehot.transform(cars)\n",
        "cars.select(\"type\",\"type_idx\",\"typx_dummy\").distinct().sort('type_idx').show()\n",
        "\n",
        "\n",
        "from pyspark.mllib.linalg import DenseVector, SparseVector\n",
        "\"dense vector: reflect 0 explicitly in a list\"\n",
        "\"sparse vector: reflect non-0 values with locations\"\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKsTPUJdVnm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"Use Case - Flight\"\n",
        "# Import the one hot encoder class\n",
        "from pyspark.ml.feature import OneHotEncoderEstimator\n",
        "\n",
        "# Create an instance of the one hot encoder\n",
        "onehot = OneHotEncoderEstimator(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
        "\n",
        "# Apply the one hot encoder to the flights data\n",
        "onehot = onehot.fit(flights)\n",
        "flights_onehot = onehot.transform(flights)\n",
        "\n",
        "# Check the results\n",
        "flights_onehot.select('org', 'org_idx', 'org_dummy').distinct().sort('org_idx').show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm4JUxgYWTFs",
        "colab_type": "text"
      },
      "source": [
        "##Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xULQy9BFWmhw",
        "colab_type": "text"
      },
      "source": [
        "Minimize the loss function: MSE = 1/n sum(yi - yi_hat)^2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J7TgsqJWRCu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Assemble predictors\"\"\"\n",
        "\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "regression = LinearRegression(labelCol = 'consumption')\n",
        "\n",
        "regression = regression.fit(cars_train)\n",
        "predictions = regression.transform(cars_train)\n",
        "\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "#Find RMSE\n",
        "RegressionEvaluator(labelCol = 'consumption').evaluate(predictions)\n",
        "#mae, r2, mse\n",
        "\n",
        "regression.intercept\n",
        "\n",
        "regression.coefficients\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4imYeST2ZBON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Create a regression object and train on training data\n",
        "regression = LinearRegression(labelCol = 'duration').fit(flights_train)\n",
        "\n",
        "# Create predictions for the testing data and take a look at the predictions\n",
        "predictions = regression.transform(flights_test)\n",
        "predictions.select('duration', 'prediction').show(5, False)\n",
        "\n",
        "# Calculate the RMSE\n",
        "RegressionEvaluator(labelCol = \"duration\").evaluate(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJyVJPJba1Yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Create a regression object and train on training data\n",
        "regression = LinearRegression(labelCol = 'duration').fit(flights_train)\n",
        "\n",
        "# Create predictions for the testing data\n",
        "predictions = regression.transform(flights_test)\n",
        "\n",
        "# Calculate the RMSE on testing data\n",
        "RegressionEvaluator(labelCol = 'duration').evaluate(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yoj8_U77cAWv",
        "colab_type": "text"
      },
      "source": [
        "##Bucketing & Engineering\n",
        "\n",
        "Operations on a single column: log(), sqrt(), pow()\n",
        "\n",
        "operations on two columns: product, ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt7_H8yXa1aO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import Bucketizer\n",
        "\n",
        "bucketizer = Bucketizer(splits = [3500,4500,6000,6500], inputCols = \"rpm\",outputCols = 'rpm_bin')\n",
        "\n",
        "cars = bucketizer.transform(cars)\n",
        "\n",
        "#operations on two columns\n",
        "cars = cars.withColumn('density_line', cars.mass/cars.length)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IV3YYWyftqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import Bucketizer, OneHotEncoderEstimator\n",
        "\n",
        "# Create buckets at 3 hour intervals through the day\n",
        "buckets = Bucketizer(splits=[0, 3, 6, 9, 12, 15, 18, 21, 24], inputCol='depart', outputCol='depart_bucket')\n",
        "\n",
        "# Bucket the departure times\n",
        "bucketed = buckets.transform(flights)\n",
        "bucketed.select('depart', 'depart_bucket').show(5)\n",
        "\n",
        "# Create a one-hot encoder\n",
        "onehot = OneHotEncoderEstimator(inputCols=['depart_bucket'], outputCols=['depart_dummy'])\n",
        "\n",
        "# One-hot encode the bucketed departure times\n",
        "flights_onehot = onehot.fit(bucketed).transform(bucketed)\n",
        "flights_onehot.select('depart', 'depart_bucket', 'depart_dummy').show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI9KjFeRhYN6",
        "colab_type": "text"
      },
      "source": [
        "##Regularization\n",
        "\n",
        "Minimize the loss function: MSE = 1/n sum(yi - yi_hat)^2 +lambda(beta)\n",
        "penalize the model with too many variables\n",
        "\n",
        "Lasso = absolute value of the coefficients; Ridge - square of the coefficient\n",
        "\n",
        "lambda = 0 --- No Regularization\n",
        "lambda = infinity -- complete regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5HwJvsOhZMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "ridge = LinearRegression(labelCol = 'consumption',elasticNetParam = 0, regParam = 0.1)\n",
        "ridge.fit(cars_train)\n",
        "\n",
        "lasso = LinearRegression(labelCol = 'consumption',elasticNetParam = 1, regParam = 0.1)\n",
        "lasso.fit(cars_train)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIrXvsczkm0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Fit Lasso model (α = 1) to training data\n",
        "regression = LinearRegression(labelCol = 'duration', regParam = 1, elasticNetParam=1).fit(flights_train)\n",
        "\n",
        "# Calculate the RMSE on testing data\n",
        "\n",
        "rmse = RegressionEvaluator(labelCol = 'duration').evaluate(regression.transform(flights_test))\n",
        "print(\"The test RMSE is\", rmse)\n",
        "\n",
        "# Look at the model coefficients\n",
        "coeffs = regression.coefficients\n",
        "print(coeffs)\n",
        "\n",
        "# Number of zero coefficients\n",
        "zero_coeff = sum([0 for beta in regression.coefficients])\n",
        "print(\"Number of ceofficients equal to 0:\", zero_coeff)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etgZ_mSYkr_E",
        "colab_type": "text"
      },
      "source": [
        "##Ensemble and Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEP_7s_PkteY",
        "colab_type": "text"
      },
      "source": [
        "##Pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LFHdRixlZGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline(stages = [indexer, onehot, assemble, regression])\n",
        "\n",
        "pipeline = pipeline.fit(Cars_train)\n",
        "\n",
        "prediction = pipeline.transform(cars_test)\n",
        "\n",
        "pipeline.stage[3]\n",
        "\n",
        "pipeline.stage[3].intercept()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwtmlzoomky3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert categorical strings to index values\n",
        "indexer = StringIndexer(inputCol = 'org',outputCol = 'org_idx')\n",
        "\n",
        "# One-hot encode index values\n",
        "onehot = OneHotEncoderEstimator(\n",
        "    inputCols=['org_idx','dow'],\n",
        "    outputCols=['org_dummy','dow_dummy']\n",
        ")\n",
        "\n",
        "# Assemble predictors into a single column\n",
        "assembler = VectorAssembler(inputCols=['km','org_dummy','dow_dummy'], outputCol='features')\n",
        "\n",
        "# A linear regression object\n",
        "regression = LinearRegression(labelCol='duration')\n",
        "\n",
        "\n",
        "# Import class for creating a pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Construct a pipeline\n",
        "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\n",
        "\n",
        "# Train the pipeline on the training data\n",
        "pipeline = pipeline.fit(flights_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "predictions = pipeline.transform(flights_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLH0wTvhnne3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "\n",
        "# Break text into tokens at non-word characters\n",
        "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
        "\n",
        "# Remove stop words\n",
        "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='terms')\n",
        "\n",
        "# Apply the hashing trick and transform to TF-IDF\n",
        "hasher = HashingTF(inputCol = remover.getOutputCol(), outputCol=\"hash\")\n",
        "idf = IDF(inputCol=hasher.getOutputCol(), outputCol=\"features\")\n",
        "\n",
        "# Create a logistic regression object and add everything to a pipeline\n",
        "logistic = LogisticRegression()\n",
        "pipeline = Pipeline(stages=[tokenizer, remover, hasher, idf, logistic])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cYFBMLUnrOW",
        "colab_type": "text"
      },
      "source": [
        "##Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL69g7kdnqiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "params =  ParamGridBuilder().build\n",
        "\n",
        "cv = CrossValidator(estimator= regression, estimatorParamMaps = params, evaluator = evaluator, numFolds = 10, seed = 13)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm3KTpmlaSC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv.cv.fit(cars_train)\n",
        "\n",
        "cv.avgMetrics #average RMSE\n",
        "\n",
        "evaluator.evaluate(cv.transform(Cars_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gB1TKwObiiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an empty parameter grid\n",
        "params = ParamGridBuilder().build()\n",
        "\n",
        "# Create objects for building and evaluating a regression model\n",
        "regression = LinearRegression(labelCol='duration')\n",
        "evaluator = RegressionEvaluator(labelCol = 'duration')\n",
        "\n",
        "# Create a cross validator\n",
        "cv = CrossValidator(estimator=regression, estimatorParamMaps=params, evaluator=evaluator, numFolds= 5)\n",
        "\n",
        "# Train and test model on multiple folds of the training data\n",
        "cv = cv.fit(flights_train)\n",
        "\n",
        "# NOTE: Since cross-valdiation builds multiple models, the fit() method can take a little while to complete."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaFRbqmmcxAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an indexer for the org field\n",
        "indexer = StringIndexer(inputCol = \"org\", outputCol = 'org_idx')\n",
        "\n",
        "# Create an one-hot encoder for the indexed org field\n",
        "onehot = OneHotEncoderEstimator(inputCols= ['org_idx'], outputCols = ['org_dummy'])\n",
        "\n",
        "# Assemble the km and one-hot encoded fields\n",
        "assembler = VectorAssembler(inputCols = ['km','org_dummy'], outputCol = 'features')\n",
        "\n",
        "# Create a pipeline and cross-validator.\n",
        "regression = LinearRegression(labelCol= ' duration')\n",
        "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\n",
        "cv = CrossValidator(estimator=pipeline,\n",
        "          estimatorParamMaps=params,\n",
        "          evaluator=evaluator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwmTqq69cyuP",
        "colab_type": "text"
      },
      "source": [
        "##Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQW710Atc0oF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "regression = LinearRegression(labelCol = 'consumption', fitIntercept = True)\n",
        "regression = regression.fit(cars_train)\n",
        "\n",
        "#RMSE\n",
        "evaluator = evaluate(regression.transform(cars_test))\n",
        "\n",
        "from pyspark.ml.tuning import PramGridBuilder\n",
        "\n",
        "params = ParamGridBuilder()\n",
        "params = params.addGrid(regression.fitIntercept, [True, False])\n",
        "\n",
        "params = params.build\n",
        "\n",
        "cv.bestModel\n",
        "predictions = cv.transform(cars_test)\n",
        "\n",
        "cv.bestModel.explainParam('fitIntercept')\n",
        "\n",
        "params = ParamGridBuilder().addGrid(regression.fitIntercept, [True, False]\\\n",
        "                                    .addGrid(regression.regParam, [0.001,0.01])\\\n",
        "                                    .addGrid(regression.elasticNetParam, [0,0.25,0.5,0.75,1])\\\n",
        "                                    .build()\n",
        "                                    \n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXxJy6qefFrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create parameter grid\n",
        "params = ParamGridBuilder()\n",
        "\n",
        "# Add grids for two parameters\n",
        "params = params.addGrid(regression.regParam, [0.01,0.1,1,10]) \\\n",
        "               .addGrid(regression.elasticNetParam, [0.0,0.5,1.0])\n",
        "\n",
        "# Build the parameter grid\n",
        "params = params.build()\n",
        "print('Number of models to be tested: ', len(params))\n",
        "\n",
        "# Create cross-validator\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=params, evaluator= evaluator, numFolds=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37xJcVSQhBQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the best model from cross validation\n",
        "best_model = cv.bestModel\n",
        "\n",
        "# Look at the stages in the best model\n",
        "print(best_model.stages)\n",
        "\n",
        "# Get the parameters for the LinearRegression object in the best model\n",
        "best_model.stages[3].extractParamMap()\n",
        "\n",
        "# Generate predictions on testing data using the best model then calculate RMSE\n",
        "predictions = best_model.transform(flights_test)\n",
        "evaluator.evaluate(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQNDqcMUiQj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create parameter grid\n",
        "params = ParamGridBuilder()\n",
        "\n",
        "# Add grid for hashing trick parameters\n",
        "params = params.addGrid(hasher.numFeatures, [1024,4096,16384]) \\\n",
        "               .addGrid(hasher.binary,[True, False])\n",
        "\n",
        "# Add grid for logistic regression parameters\n",
        "params = params.addGrid(logistic.regParam, [0.01,0.1,1,10]) \\\n",
        "               .addGrid(logistic.elasticNetParam, [0,0.5,1])\n",
        "\n",
        "# Build parameter grid\n",
        "params = params.build()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNBq3ITYiZP9",
        "colab_type": "text"
      },
      "source": [
        "##Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c67-bKskiY39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "forest=RandomForestClassifier(numTrees = 5)\n",
        "\n",
        "forest = forest.fit(cars_train)\n",
        "\n",
        "forest.featureImportances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwuiOOpdkSZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "\n",
        "gbt = GBTClassifier(maxIter = 10)\n",
        "\n",
        "gbt = gbt.fit(cars_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PpOsHRknmXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a random forest classifier\n",
        "forest = RandomForestClassifier()\n",
        "\n",
        "# Create a parameter grid\n",
        "params = ParamGridBuilder().addGrid(forest.featureSubsetStrategy, ['all', 'onethird', 'sqrt', 'log2']) \\\n",
        "            .addGrid(forest.maxDepth, [2, 5, 10]) \\\n",
        "            .build()\n",
        "\n",
        "# Create a binary classification evaluator\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "\n",
        "# Create a cross-validator\n",
        "cv = CrossValidator(estimator = forest, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP-lifSzqSZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Average AUC for each parameter combination in grid\n",
        "avg_auc = cv.avgMetrics\n",
        "\n",
        "# Average AUC for the best model\n",
        "best_model_auc =  max(avg_auc)\n",
        "\n",
        "# What's the optimal parameter value?\n",
        "opt_max_depth = cv.bestModel.explainParam('maxDepth')\n",
        "opt_feat_substrat = cv.bestModel.explainParam('featureSubsetStrategy')\n",
        "\n",
        "# AUC for best model on testing data\n",
        "best_auc = evaluator.evaluate(cv.transform(flights_test))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}